{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Local imports\n",
    "from utils.elasticsearch_utils import get_all_hits, flatten_dict, flatten_dict_keys, \\\n",
    "    get_hits_dsl_query, \\\n",
    "    get_hits_dict_query, update_fields_select_df, get_server_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Useful paths\n",
    "OUT_DIR = \"results/reports\"\n",
    "eDATA_FILTER_FILE = \"configs/edata_fields.csv\"\n",
    "rDATA_FILTER_FILE = \"configs/reporting_fields.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Example of elasticsearch data fetch\n",
    "\n",
    "# -- Simple Gather all data from elasticsearch\n",
    "full_edata: List[Dict] = get_all_hits()\n",
    "\n",
    "edata: List[Dict] = get_hits_dsl_query({\n",
    "    \"match\": {\n",
    "        \"args.title\": {\n",
    "            \"query\": \"andrei\",\n",
    "            \"type\": \"phrase\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "df.to_csv(\"3dDataset\")\n",
    "edata: List[Dict] = get_hits_dict_query({\n",
    "    \"args.title\": [\"andrei\"], \n",
    "    \"args.experiment\": [\"3d_datasets_ewc_256_mlp\"]\n",
    "}, exclude_keys=[\"task_info\", ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Simple fetch of report from server \n",
    "rdata: List[Dict] = get_server_reports(e_ids=[\"E1sZ62MBm5wd3rDHL-EF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Key filters for reports or eData\n",
    "rdata_keys = [] # Empty if you do not want to update keys\n",
    "edata_keys = []\n",
    "\n",
    "# eDATA columns read \n",
    "edata_keys = sorted(list(flatten_dict_keys(edata[0])))  # Use tu update / initialize\n",
    "\n",
    "# Reporting columns read \n",
    "rdata_keys = sorted(list(flatten_dict_keys(rdata[0])))  # Use tu update / initialize\n",
    "\n",
    "# -- Read /update them to file\n",
    "# Read key filters and smarg_grop prop for eData \n",
    "e_select_df, e_select_k, e_select_sg = update_fields_select_df(None, edata_keys, \n",
    "                                                               update_file=eDATA_FILTER_FILE)\n",
    "\n",
    "# Read key filters and smarg_grop prop for raw reporting data \n",
    "r_select_df, r_select_k, r_select_sg = update_fields_select_df(None, rdata_keys, \n",
    "                                                               update_file=rDATA_FILTER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "edata: List[Dict] = get_hits_dict_query({\n",
    "    \"args.experiment\": [\"3d_datasets_ewc_mlp_multi\"]}, include_keys=e_select_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to dataframe\n",
    "\n",
    "data = [flatten_dict(x) for x in edata]\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Gathering raw data from server\n",
    "\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "full_report, df_return = get_server_reports(experiments=[\"3d_datasets_ewc_mlp_multi\"],\n",
    "                                            include_keys=r_select_k, smart_group=r_select_sg,\n",
    "                                            df_format=True)\n",
    "df, report_info = df_return\n",
    "\n",
    "# Selecting finished experiment\n",
    "# TODO next repair -> new argument with argument_finished\n",
    "# For this experiment they should have reached same seen value\n",
    "max_seen = df.groupby([\"reporting_idx\"]).max()[\"seen\"]\n",
    "select_reports = max_seen[max_seen == max_seen.max()].index\n",
    "df = df[df[\"reporting_idx\"].apply(lambda x: x in select_reports)]\n",
    "df.isnull().any() # Check if any with null value\n",
    "\n",
    "#  df_task[\"c_reporting_idx\"] = df_task.groupby([\"seen\", optim_key]).cumcount()\n",
    "\n",
    "# Print Columns \n",
    "print(df.columns)\n",
    "\n",
    "df.groupby([\"reporting_idx\", \"seen\"]).count()[\"task_idx\"].unique()\n",
    "\n",
    "optim_key = \"_args.train._optimizer.__name.\"\n",
    "acc_key = '_eval_trace.0.acc.2'\n",
    "merge_e_key = '_args.lifelong.merge_elasticities.'\n",
    "dataset_key = '_args.tasks.datasets.'\n",
    "reporting_idx_key = \"reporting_idx\"\n",
    "run_id_key = '_args.run_id.'\n",
    "batch_size_key = \"_args.train.batch_size.\"\n",
    "scale_key = '_args.lifelong.scale.'\n",
    "optimizers = df[optim_key].unique()\n",
    "seen_idxs = df[\"seen\"].unique()\n",
    "\n",
    "# Make some filters\n",
    "df_select = df.copy()\n",
    "df_select = df_select[df_select[dataset_key].apply(lambda x: x == ['cifar10', 'fashion', 'mnist'])]\n",
    "df_select = df_select[df_select[merge_e_key] == True]\n",
    "df_select = df_select[df_select[optim_key] == 'Adam 0.001']\n",
    "\n",
    "# Set normal info\n",
    "condition = \"task_idx\"\n",
    "\n",
    "# Make special column column :)\n",
    "condition = \"combination\"\n",
    "df_select[condition] = df_select[\"task_idx\"].apply(str) + \"_\"\\\n",
    "                       + df_select[optim_key].apply(str) + \"_\"\\\n",
    "                       # + df_select[scale_key].apply(str)\\\n",
    "                       # + df_select[batch_size_key].apply(str)\n",
    "\n",
    "df_tsplot = df_select.groupby([\"seen\", condition, run_id_key]).max()[acc_key]\n",
    "df_tsplot = df_tsplot.reset_index()\n",
    "\n",
    "ax = sns.tsplot(time=\"seen\", value=acc_key, condition=condition, unit=run_id_key,\n",
    "                data=df_tsplot)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weight importance\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "experiment_path = \"/media/andrei/CE04D7C504D7AF291/rl/lifelong-learning/xdata/1530208894_test_weight_importance\"\n",
    "reports = glob.glob(experiment_path + \"/**/reporting.pkl\", recursive=True)\n",
    "full_data  = [torch.load(x, map_location=lambda storage, loc: storage) for x in reports]\n",
    "all_data = [x[\"_task_train_tick\"][0][\"info\"] for x in full_data]\n",
    "all_data_acc = [x[\"_task_train_tick\"][0][\"task\"][0][\"acc\"] for x in full_data]\n",
    "data = all_data[1]\n",
    "\n",
    "# key names\n",
    "key_mode = \"mode\"\n",
    "key_constraints = \"constraint\"\n",
    "key_res_size = \"res_size\"\n",
    "key_results = \"results\"\n",
    "\n",
    "# -- Example data \n",
    "\n",
    "# Consider 1 single model 1 data set trained ( => 1 task train tick)\n",
    "mode = data[key_mode]\n",
    "\n",
    "# {param_name: torch.tensor(torch.Size(param))\n",
    "constraints = data[key_constraints]\n",
    "\n",
    "# res_size: no_segments, no_samples\n",
    "res_size = data[key_res_size]\n",
    "no_segments, no_samples = res_size\n",
    "\n",
    "# {params_name: {acc: torch.tensor(torch.Size(param) + res_size), loss: ... } }\n",
    "results = data[key_results]\n",
    "\n",
    "print(constraints.keys())\n",
    "\n",
    "# -- Plot constraint values stats\n",
    "plot_data = []\n",
    "for ix, constraint in enumerate([x[key_constraints] for x in all_data]):\n",
    "    y = []\n",
    "    x = []\n",
    "    for param_name, constraint in constraint.items():\n",
    "        y += constraint.view(-1).cpu().numpy().tolist()\n",
    "        x += (constraint.numel() * [param_name])\n",
    "    trace = go.Box(y=y, x=x, name=str(ix))\n",
    "    plot_data.append(trace)\n",
    "    \n",
    "layout = go.Layout(\n",
    "    yaxis=dict(\n",
    "        title='normalized moisture',\n",
    "        zeroline=False\n",
    "    ),\n",
    "    boxmode='group'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=plot_data, layout=layout)\n",
    "py.plot(fig, auto_open=False)\n",
    "\n",
    "# Scatter plot value (e.g. acc) vs noise \n",
    "no_layers = len(constraints.keys())\n",
    "\n",
    "table_data = dict({k: [] for k in constraints.keys()})\n",
    "\n",
    "for pi, param_name in enumerate(constraints.keys()):\n",
    "    td = []\n",
    "    for ix in range(len(all_data)):\n",
    "        constraint = all_data[ix][key_constraints][param_name].view(-1).unsqueeze(0)\\\n",
    "            .unsqueeze(2).unsqueeze(2).detach()\n",
    "        results = all_data[ix][key_results][param_name][\"acc\"].view(-1, no_segments, no_samples).detach()\n",
    "        results = results.permute([1, 0, 2]).unsqueeze(3)\n",
    "        results = results.div(all_data_acc[ix])\n",
    "        constraint = constraint.expand_as(results)\n",
    "        td.append(torch.cat([results, constraint], dim=3))\n",
    "    td = torch.cat(td, dim=2)\n",
    "    table_data[param_name] = td\n",
    "\n",
    "# table_data: {param_name : torch.tensor(\n",
    "    # no_segments, weights, \n",
    "    # samples(no_samples * no_experiments), \n",
    "    # (acc, constraint)\n",
    "    # }}\n",
    "\n",
    "param_name = \"fc.0.weight\"\n",
    "for i in range(results.size(0)):\n",
    "    if len(results[i, :].unique()) > 1:\n",
    "        print(results[i, :].unique())\n",
    "        \n",
    "td = table_data[\"heads.0.weight\"]\n",
    "segments = torch.linspace(0, 1, no_segments).unsqueeze(1).unsqueeze(1).expand(td.size()[:-1])\n",
    "segments = segments.contiguous()\n",
    "z = td.view(-1, 2)[:, 0].numpy()\n",
    "x = td.view(-1, 2)[:, 1].numpy()\n",
    "y = segments.view(-1).numpy()\n",
    "\n",
    "trace = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.plot(fig, filename='simple-3d-scatter', auto_open=False)\n",
    "\n",
    "\n",
    "\n",
    "    ax.scatter(x, y, c=color, s=scale, label=color, alpha=0.3, edgecolors='none')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
